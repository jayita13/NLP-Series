{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Series - Part 1\n",
    "## Text Preprocessing Techniques\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Natural Language Toolkit\n",
    "#### NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Importing NLTK & downloading all libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Note - The NLTK download manager GUI opens up, click on 'all' (First option at top), & click on Download\n",
    "###### this will download all nltk packages to local\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a paragraph of text to perform actions\n",
    "paragraph = \"Sunset is the time of day when our sky meets the outer space solar winds. There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind. The sun moves slowly to hide behind the line of horizon, while the moon races to take its place in prominence atop the night sky. People slow to a crawl, entranced, fully forgetting the deeds that must still be done. There is a coolness, a calmness, when the sun does set.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizing - NLTK Tokenizer Package\n",
    "\n",
    "#### Tokenizers divide strings into lists of substrings. For example, tokenizers can be used to find the words and punctuation in a string:\n",
    "\n",
    "##### https://www.nltk.org/api/nltk.tokenize.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenizing sentences\n",
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sunset is the time of day when our sky meets the outer space solar winds.',\n",
       " 'There are blue, pink, and purple swirls, spinning and twisting, like clouds of balloons caught in a whirlwind.',\n",
       " 'The sun moves slowly to hide behind the line of horizon, while the moon races to take its place in prominence atop the night sky.',\n",
       " 'People slow to a crawl, entranced, fully forgetting the deeds that must still be done.',\n",
       " 'There is a coolness, a calmness, when the sun does set.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizing words\n",
    "words = nltk.word_tokenize(paragraph)\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sunset',\n",
       " 'is',\n",
       " 'the',\n",
       " 'time',\n",
       " 'of',\n",
       " 'day',\n",
       " 'when',\n",
       " 'our',\n",
       " 'sky',\n",
       " 'meets',\n",
       " 'the',\n",
       " 'outer',\n",
       " 'space',\n",
       " 'solar',\n",
       " 'winds',\n",
       " '.',\n",
       " 'There',\n",
       " 'are',\n",
       " 'blue',\n",
       " ',',\n",
       " 'pink',\n",
       " ',',\n",
       " 'and',\n",
       " 'purple',\n",
       " 'swirls',\n",
       " ',',\n",
       " 'spinning',\n",
       " 'and',\n",
       " 'twisting',\n",
       " ',',\n",
       " 'like',\n",
       " 'clouds',\n",
       " 'of',\n",
       " 'balloons',\n",
       " 'caught',\n",
       " 'in',\n",
       " 'a',\n",
       " 'whirlwind',\n",
       " '.',\n",
       " 'The',\n",
       " 'sun',\n",
       " 'moves',\n",
       " 'slowly',\n",
       " 'to',\n",
       " 'hide',\n",
       " 'behind',\n",
       " 'the',\n",
       " 'line',\n",
       " 'of',\n",
       " 'horizon',\n",
       " ',',\n",
       " 'while',\n",
       " 'the',\n",
       " 'moon',\n",
       " 'races',\n",
       " 'to',\n",
       " 'take',\n",
       " 'its',\n",
       " 'place',\n",
       " 'in',\n",
       " 'prominence',\n",
       " 'atop',\n",
       " 'the',\n",
       " 'night',\n",
       " 'sky',\n",
       " '.',\n",
       " 'People',\n",
       " 'slow',\n",
       " 'to',\n",
       " 'a',\n",
       " 'crawl',\n",
       " ',',\n",
       " 'entranced',\n",
       " ',',\n",
       " 'fully',\n",
       " 'forgetting',\n",
       " 'the',\n",
       " 'deeds',\n",
       " 'that',\n",
       " 'must',\n",
       " 'still',\n",
       " 'be',\n",
       " 'done',\n",
       " '.',\n",
       " 'There',\n",
       " 'is',\n",
       " 'a',\n",
       " 'coolness',\n",
       " ',',\n",
       " 'a',\n",
       " 'calmness',\n",
       " ',',\n",
       " 'when',\n",
       " 'the',\n",
       " 'sun',\n",
       " 'does',\n",
       " 'set',\n",
       " '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming \n",
    "##### Stemming is the process of producing morphological variants of a root/base word. Stemming programs are commonly referred to as stemming algorithms or stemmers. A stemming algorithm reduces the words “chocolates”, “chocolatey”, “choco” to the root word, “chocolate” and “retrieval”, “retrieved”, “retrieves” reduce to the stem “retrieve”. Stemming is faster than lemmatization.\n",
    "\n",
    "##### Some more example of stemming for root word \"like\" include: -> \"likes\" , \"liked\" , \"likely\" , \"liking\"\n",
    "\n",
    "##### Stemming may or may not produce meaning words, hence can be used in applications like Sentiment Analysis. Two kinds of stemming - Understemming & Overstemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### https://www.nltk.org/api/nltk.stem.porter.html\n",
    "\n",
    "##### Stop Words: A stop word is a commonly used word (such as “the”, “a”, “an”, “in”) that a search engine has been programmed to ignore, both when indexing entries for searching and when retrieving them as the result of a search query. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sunset time day sky meet outer space solar wind .\n",
      "blue , pink , purpl swirl , spin twist , like cloud balloon caught whirlwind .\n",
      "sun move slowli hide behind line horizon , moon race take place promin atop night sky .\n",
      "peopl slow crawl , entranc , fulli forget deed must still done .\n",
      "cool , calm , sun set .\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(sentences)):\n",
    "    word_list = nltk.word_tokenize(sentences[i])\n",
    "    word_list = [stemmer.stem(word.lower()) for word in word_list if word.lower() not in set(stopwords.words('english'))]\n",
    "    new_sentences = \" \".join(word_list)\n",
    "    print(new_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization \n",
    "\n",
    "##### Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma\n",
    "\n",
    "##### Lemmatization produces meaningful words & thus requires more computation time. This can be mostly used in chatbots, Q&A systems, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sunset time day sky meet outer space solar wind .\n",
      "blue , pink , purple swirl , spinning twisting , like cloud balloon caught whirlwind .\n",
      "sun move slowly hide behind line horizon , moon race take place prominence atop night sky .\n",
      "people slow crawl , entranced , fully forgetting deed must still done .\n",
      "coolness , calmness , sun set .\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(sentences)):\n",
    "    word_lemma = nltk.word_tokenize(sentences[i])\n",
    "    word_lemma = [lemmatizer.lemmatize(word.lower()) for word in word_lemma if word.lower() not in set(stopwords.words('english'))]\n",
    "    lemma_sentences = \" \".join(word_lemma)\n",
    "    print(lemma_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag Of Words\n",
    "\n",
    "##### The bag-of-words model is a simplifying representation used in natural language processing and information retrieval (IR). In this model, a text (such as a sentence or a document) is represented as the bag (multiset) of its words, disregarding grammar and even word order but keeping multiplicity. In other words it is way of numerical representation which can be fed to model. Two kinds of BOW representation - Normal BOW & Binary BOW(matrix sentence-word frequency). Useful for small datasets only.\n",
    "\n",
    "##### Disadvantage - words can have same semantics(frequency), hence model can get confused which one should have more weightage. Fails for large datasets.\n",
    "\n",
    "###### (1) John likes to watch movies. Mary likes movies too.\n",
    "###### (2) Mary also likes to watch football games.\n",
    "\n",
    "###### BoW1 = {\"John\":1,\"likes\":2,\"to\":1,\"watch\":1,\"movies\":2,\"Mary\":1,\"too\":1};\n",
    "###### BoW2 = {\"Mary\":1,\"also\":1,\"likes\":1,\"to\":1,\"watch\":1,\"football\":1,\"games\":1};"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning text\n",
    "import re\n",
    "\n",
    "corpus = []\n",
    "for i in range(len(sentences)):\n",
    "    # replacing everything(except alphabets) with space\n",
    "    cleaned = re.sub(\"[^a-zA-Z]\", ' ', sentences[i])\n",
    "    # lemmatizing\n",
    "    cleaned = cleaned.lower()\n",
    "    cleaned = cleaned.split()   # list of words\n",
    "    cleaned = [lemmatizer.lemmatize(word) for word in cleaned if word not in set(stopwords.words('english'))]\n",
    "    cleaned = \" \".join(cleaned)\n",
    "    corpus.append(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating BoW model\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(max_features=1500)\n",
    "model = cv.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,\n",
       "        0, 0, 1],\n",
       "       [0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "        1, 1, 0],\n",
       "       [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1,\n",
       "        0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n",
       "        0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TFIDF\n",
    "\n",
    "##### In information retrieval, tf–idf (also TF*IDF, TFIDF, TF–IDF, or Tf–idf), short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.\n",
    "\n",
    "###### Term frequency tf(t,d), is the relative frequency of term t within document d,\n",
    "\n",
    "###### The inverse document frequency is a measure of how much information the word provides, i.e., if it is common or rare across all documents. It is the logarithmically scaled inverse fraction of the documents that contain the word (obtained by dividing the total number of documents by the number of documents containing the term, and then taking the logarithm of that quotient):\n",
    "\n",
    "![tf-df](https://cdn-media-1.freecodecamp.org/images/1*q3qYevXqQOjJf6Pwdlx8Mw.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating tfidf model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "cv = TfidfVectorizer(max_features=1500)\n",
    "model = cv.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.3399922 ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.3399922 ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.3399922 ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.27430356, 0.        , 0.        ,\n",
       "        0.3399922 , 0.3399922 , 0.        , 0.        , 0.        ,\n",
       "        0.3399922 , 0.        , 0.        , 0.3399922 , 0.        ,\n",
       "        0.        , 0.3399922 ],\n",
       "       [0.        , 0.30151134, 0.        , 0.30151134, 0.        ,\n",
       "        0.30151134, 0.30151134, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.30151134, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.30151134, 0.        , 0.        , 0.30151134,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.30151134, 0.        , 0.        ,\n",
       "        0.        , 0.30151134, 0.        , 0.        , 0.30151134,\n",
       "        0.30151134, 0.        ],\n",
       "       [0.264426  , 0.        , 0.264426  , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.264426  , 0.264426  , 0.        , 0.264426  , 0.        ,\n",
       "        0.264426  , 0.264426  , 0.        , 0.264426  , 0.        ,\n",
       "        0.        , 0.        , 0.264426  , 0.264426  , 0.        ,\n",
       "        0.264426  , 0.        , 0.21333723, 0.        , 0.264426  ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.21333723,\n",
       "        0.        , 0.        , 0.264426  , 0.        , 0.        ,\n",
       "        0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.31622777, 0.        ,\n",
       "        0.31622777, 0.31622777, 0.31622777, 0.31622777, 0.31622777,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.31622777, 0.        , 0.        ,\n",
       "        0.31622777, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.31622777, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.31622777, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.52335825,\n",
       "        0.        , 0.        , 0.52335825, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.52335825, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.42224214,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        ]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f791463d7aacf3cab3bf5c25c40d85905c8079d9b4c874ca8e5a7982f517920d"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
